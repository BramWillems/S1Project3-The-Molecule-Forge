{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e92007d1",
   "metadata": {},
   "source": [
    "# Project 3 - The molecule forge\n",
    "\n",
    "Dit is mijn jupyter notebook over mijn 3e project, in dit project ga ik werken met AI en proberen om nieuwe fictieve moleculen te maken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25053877",
   "metadata": {},
   "source": [
    "Tips voor mijzelf\n",
    "- Numpy arrays voor speed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a247812",
   "metadata": {},
   "source": [
    "### Het maken van een Virtual Environment\n",
    "Voor coderen in python is het belangrijk om een Virtual Environment te maken, hiermee kun je per project een eigen python install hebben als het ware. \n",
    "Er zijn een aantal redenen om een Venv te gebruiken:\n",
    " - Scheiden van projecten, bijvoorbeeld bij het ene project heb je numpy versie 1.5 nodig en bij een ander project heb je versie 2 nodig. Doormiddel van Venv's kun je dit dan makkelijk scheiden.\n",
    " - Voorkomt vervuiling van je environment, er zijn geen onnodige packages die mee worden gestuurd als het project word geplubliceerd.\n",
    " - Een Venv is makkelijk te maken op een ander systeem waar de packages niet zijn geinstalleerd doormiddel van een requirements.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861c56e5",
   "metadata": {},
   "source": [
    "Stukje voor imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e12e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "\n",
    "from itertools import chain\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e10b672",
   "metadata": {},
   "source": [
    "Dit is een functie die ik zelf heb gemaakt om de eerst n aantal regels in een bestand te openen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1fa734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lines(n):\n",
    "    lines = []\n",
    "    with open(\"guacamol_v1_train.smiles\", \"r\") as file:\n",
    "        for l in range(n):\n",
    "            line = file.readline().strip()\n",
    "            if not line:\n",
    "                break\n",
    "            lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb135520",
   "metadata": {},
   "source": [
    "Stukje code om de eerste x aantal codes op te slaan en te printen naar de console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4280dc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CCC(C)(C)Br', 'CCCN(CCc1cccc(-c2ccccc2)c1)C(=O)C1OC(C(=O)O)=CC(N)C1NC(C)=O', 'Oc1ccc(C2CC(c3ccccc3)=NN2C(=S)Nc2ccccc2)cc1', 'CC1(C)OCC2OC3(C4OC(C)(C)OC4CO)OC(C)(C)OC3C2O1', 'COC(=O)c1cc(C(=CCCCC(=O)SC)c2cc(Cl)c(OC)c(C(=O)OC)c2)cc(Cl)c1OC', 'Cc1cc(COc2ccc(NC(=O)C3CN(C)CCC3C(=O)NO)cc2)c2ccccc2n1', 'CCOC(=O)c1ccc(O)c(-n2cc3c(c2-c2ccccc2)c(=O)n(C)c(=O)n3C)c1', 'COc1cc(OC)c2c(-c3cccc(-c4ccc(C#N)cc4)c3)cc(=O)oc2c1', 'COc1cc2[nH]c(C(=O)Nc3ccc(F)cc3)cc2c(OC)c1OC', 'COc1cc(F)cc(-c2ccc(C(CC(=O)O)NC(=O)C3CCCN3S(=O)(=O)c3cc(Cl)cc(Cl)c3)cc2)c1', 'COc1cc(-c2cc(OC)c(-n3c(=O)ccc4cc(S(=O)(=O)Nc5cccnn5)ccc43)cc2F)ccc1Cl', 'CCCCC1(C)CC(CO)C(CCCC)(OC)OO1', 'CCCCCCCCCCCCNc1ccc2c3c(cccc13)C(=O)N(CCCN1CCCNCCNCCCNCC1)C2=O', 'COC(=O)C1=C(c2cc3ccccc3o2)CC2CCC1N2C(=O)NCc1cccc2ccccc12', 'Cc1ccccc1C=Cc1cccc(C(F)(F)P(=O)(O)O)c1', 'Nc1cc(C2CCNCC2)cc(NC2CCCC2)n1', 'CCN(C(C)=O)c1ccc(OC)c2nc(NC(=O)C3CCN(C(=O)c4cccc(C(F)(F)F)c4)CC3)sc12', 'COC(=O)C1CCC(C)C(c2ccc(C)cc2)N1C(=O)c1ccc(C=NOCC(O)COCc2ccco2)cc1', 'CCOC(=O)C1(Cc2ccccc2C)CCCN(C(=O)CCC(=O)OC)C1', 'O=C1Nc2ccccc2C(=O)N2C=C(c3ccc(Cl)c(Cl)c3)CC12', 'CCCn1c(CN2C(=O)COc3c2cc(C)cc3[N+](=O)[O-])nnc1-c1ccc(Cl)cn1', 'Cc1cccc(NC(=O)c2cc(-c3cccc(F)c3)ccn2)n1', 'N=C(N)Nc1cccnc1C1CC(O)C(NC(=N)N)CC1NC(=N)N', 'CCOC(=O)CCc1c(C)nc2c(c(C)nn2-c2ccc(C)cc2)c1C', 'CON(C)S(=O)(=O)c1ccc2c(c1)N(Cc1ccc(C)cc1)C(=O)C1CCCN21', 'Cc1ccccc1-c1cc(C(OCc2cccc(F)c2)c2cncn2C)ccc1C#N', 'COc1ccc(C23Oc4cc(OC)cc(OC)c4C2(O)C(O)C(C(=O)NOCCO)C3c2ccccc2)cc1', 'COc1cccc2cc(-c3nc4c(c(=O)n(C)c(=O)n4C)n3C)oc12', 'CC(=O)NC(C)c1ccc(OC2CCN(c3ccnc(OC4CCCCC4)c3)C2)cc1', 'N=C(O)C(N=C(O)c1cnc(N2CC(F)(F)C2)c(OCC2CC2)n1)c1ccc(Cl)cc1', 'O=C(Nc1ccn(-c2ccccc2F)n1)C1CCC(Oc2cc(Cl)ccn2)CC1', 'CCOC(=O)Cc1cc(-c2ccc(C(F)(F)F)cc2)n(-c2ccc(S(C)(=O)=O)cc2)c1C', 'CN(C)CCC(CSc1ccccc1)Nc1ccc(S(=O)(=O)NC(=O)c2ccc(N3CCC4(CC3)CC(c3ccccc3)=NO4)cc2)cc1[N+](=O)[O-]', 'O=C(Nc1ccccc1)N1N=C(c2ccc(F)cc2)CC1c1ccc(F)cc1', 'C=CCc1ccccc1OCC(O)CNC1(C)CCCC(C(C)(C)NC(=O)CBr)C1', 'CN(C)c1ccc(C(=S)N2CCOCC2)cc1Br', 'COc1ccc2ccc(-c3cccnc3)c(Cl)c2c1F', 'CCCC(=O)N1CCC1(C)C(=O)NS(=O)(=O)c1cc2c(cc1C)CCCS2(=O)=O', 'Cc1noc(C)c1C(=O)N1CCC(n2cccn2)CC1', 'O=C(CCNC(=O)N1CC(=O)Nc2ccccc21)NCCc1cccc(F)c1', 'CC1(C)C(=N)NC2(c3cc(NC(=O)c4ccc(Cl)cn4)ccc3F)COCC2S1(=O)=O', 'O=C(O)C1CCC(OCC2CC(F)CN2C(=O)Cc2ccc(NC(=O)N3CCc4ccccc43)c(Cl)c2)CC1', 'NCCCCCCCCCCCCNc1ccn(C2CCC(CO)O2)c(=O)n1', 'Cc1cc(-c2cnn(C3(CO)CCCC3)c2)c2c(c1)C(O)(C(F)(F)F)c1ccccc1-2', 'COc1cc(C=NNC(=O)C(OC)c2ccc3c(c2)OCCO3)ccc1Cl', 'CC(=O)N1NC(=C2C(=O)N(C)C(=O)N(C)C2=O)CC1c1ccccc1', 'COc1c(NC(C)=O)cc(C)cc1NC(C)=O', 'O=C1Nc2ccc(Br)cc2C12ON=C(c1ccc(Cl)cc1)N2c1cccc(Cl)c1', 'CCCc1cc(=O)oc2c(C(=O)CC)c(OCC)c3c(c12)OC(C)(C)C=C3', 'COc1cccc(CNCC(C)n2c(=O)c(-c3cccc(OC)c3)c(C)n(Cc3c(F)cccc3F)c2=O)c1', 'CCN(CC)C(=S)Nc1sc(C(N)=O)c(C)c1C(=O)O', 'COc1cc(OC)cc(-c2cc3cnc(N(CCCO)C(C)=O)cc3nc2NC(=O)NC(C)(C)C)c1', 'Cc1ccc(NC(=S)OCCN2C(=O)c3ccccc3C2=O)cc1', 'O=C(COc1ccc(C=NNS(=O)(=O)c2ccccc2)cc1)NCc1ccccc1', 'CC(C)(C)C(=O)Nc1ccc(-c2cn3cccnc3n2)cc1', 'COc1ccccc1Oc1c(NS(=O)(=O)c2ccc(C(C)C)cn2)nc(N2CCOCC2)nc1OCCNS(=O)(=O)c1ccc(C)cc1', 'O=c1cc(-c2ccccc2)oc2cc(OCc3ccccc3)c(OCCCN3CCCCC3)c(O)c12', 'CCc1cc(C(=O)NC2CC(C(=O)NCCOC)N(C(=O)c3coc4ccccc34)C2)n(C)n1', 'Nc1nccc2c1ncn2C1CC(CBr)C(O)C1O', 'CC(C)(C)C(=O)NC(Cc1ccc(NC(=O)c2ccnc3ccccc23)cc1)C(=O)O', 'Cc1c(C(=O)OCC(=O)N2CCCC2=O)oc2ccccc12', 'COc1cc(C=NNc2nc3c(c(=O)n(C)c(=O)n3C)n2Cc2ccccc2)ccc1OC(=O)c1ccco1', 'COc1cn(CC(N)=O)c(CSc2nccc(C)n2)cc1=O', 'CN(C)CCOc1ccc(-n2ccnc2)cc1', 'COc1cc(-c2nnc(SCC(=O)Nc3ccc(C(C)=O)cc3)n2-c2ccccc2)cc(OC)c1OC', 'CC(C)CC(NC(=O)C(CC(C)C)NOC(=O)CN(C(=O)OCc1ccccc1)P(=O)(O)O)C(=O)O', 'c1ccc(COc2ccc(OCc3ccc4ccccc4n3)cc2C2(c3ccccc3)CC3CCC2C3)cc1', 'CC(C)(C)NCC(O)COc1ccc(-c2nc(-c3cccs3)c[nH]2)cc1', 'CCCN(Cc1cccs1)C(=O)Nc1ccc(Br)cc1F', 'COc1ccccc1C(=O)NC(=O)NC1c2ccccc2-c2ccccc21', 'Cc1ccc(C=CC(=O)Nc2ccc(N3CCN(CC(O)(Cn4cncn4)c4ccc(F)cc4F)CC3)c(F)c2)cc1', 'Nc1sc(-c2ccc(Cl)cc2)c(CN2CCN(c3ccc(C(F)(F)F)cc3)CC2)c1C(=O)c1ccc(Cl)cc1', 'Cc1ccccc1-c1nc(N)nc(O)c1Cl', 'CC(=O)Nc1cc(F)ccc1CN1Cc2c(c(O)c3ncccc3c2N(C)S(C)(=O)=O)C1=O', 'Cc1ccc(NC(=O)CCCOc2ccccc2)cc1S(=O)(=O)N1CCOCC1', 'COc1ccc(CC(=O)O)cc1-c1ccc(F)c2c1CCN(C(=O)CCN1CCCc3ccccc31)C2', 'C=CS(=O)(=O)c1cccc(Nc2cc(S(=O)(=O)O)c(N)c3c2C(=O)c2ccccc2C3=O)c1', 'c1ccc(-c2nc(-c3ccncc3)no2)cc1', 'O=C(NCCCO)c1cc(-c2ccco2)nc2ccccc12', 'Cc1nn(-c2ccccc2)c(C)c1CC(=O)NC1CC(=O)N(C)C1', 'O=C(O)CCCCCCCCc1ccc(Nc2c3ccccc3nc3ccccc23)cc1', 'COc1cc(NCc2ccc3oc(=O)c(-c4ccccc4)nc3c2)cc(OC)c1OC', 'CC(C)(C)CN1CCC2(CC1)CN(c1ccccc1N=c1[nH]nc(-c3cccc(F)c3)s1)c1c(O)ccc(C(F)(F)F)c12', 'O=C(CN(Cc1ccco1)C(=O)c1ccc(Cl)cc1)Nc1ccc(Cl)c(Cl)c1', 'C=C(C)C1CC=C(COC(C)=O)CCC(O)C(C)=CCC=C(C)CC1', 'COc1ccc(CNC(=O)c2ccc3ccccc3n2)cc1OC', 'NS(=O)(=O)c1ccccc1CC(O)CC(Cc1ccccc1)C(=O)NC1c2ccccc2CC1O', 'Cc1ccc(S(=O)(=O)NC2(C(F)(F)F)NC(=O)N(c3ccccc3F)C2=O)cc1', 'CC(C)NC(=O)c1ccc(S(=O)(=O)N(C)C)cc1', 'CC(=O)C1=NN(c2ccccc2)C2(SC(=Cc3ccccc3)C(=O)N2c2ccccc2)S1', 'OCC1OC(n2cnc3c(NCC4=CCc5ccccc54)ncnc32)C(O)C1O', 'Cc1ccc(S(=O)(=O)N=C(CC(C)Br)N(C)C)cc1', 'O=C(c1cc2cccc(F)c2[nH]1)N1CCNCC1', 'COc1cc(OC)c(C2CCN(C)CC2)c(O)c1C(=O)C=Cc1ccccc1F', 'CCN(C(=O)OCc1ccccc1)C1CCN(CCC(CN2C(=O)NC(Cc3c[nH]c4ccccc34)C2=O)c2cccc(Cl)c2)CC1', 'NCCCNCCCCNCCCNC(=O)Cc1cccc2ccccc12', 'O=C(NCCCN1CCCC1=O)C1CCN(S(=O)(=O)c2cccc(Cl)c2Cl)CC1', 'CSc1ccc(NC(=O)Nc2ccc(-c3cc(Nc4cccc(C(F)(F)F)c4)ncn3)cc2)cc1', 'COc1cc(OC)cc(C(=O)Nc2ccc(N3CCN(C(=O)c4cccs4)CC3)cc2)c1', 'Cc1ccc(C2=C(CC(=O)O)C(c3cccs3)NC(S)=N2)cc1']\n"
     ]
    }
   ],
   "source": [
    "molecules_amount = 100\n",
    "molecules = read_lines(molecules_amount)\n",
    "print(molecules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee18a03a",
   "metadata": {},
   "source": [
    "Ook moet ik een functie hebben om te kijken of de gemaakte moleculen daadwerkelijk echt bestaan, hiervoor gebruik ik de functie MolFromSmiles(), deze functie pakt een van mijn smiles moleculen en splitst deze in losse atomen. De functie plakt deze dan na elkaar en probeert deze in mol formaat van Rdkit te zetten, mocht dit niet lukken dan is dit invalid en krijg ik None terug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "471634f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_smiles(molecule):\n",
    "    mol = Chem.MolFromSmiles(molecule)\n",
    "    if mol is not None:\n",
    "        return 'Valid'\n",
    "    else:\n",
    "       return 'Not valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dc9d45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cc1c(C(=O)OCC(=O)N2CCCC2=O)oc2ccccc12\n",
      "Valid\n",
      "Cc1c(C(=O)OCC(=O)N2CCCC2=O)oc2ccccc1\n",
      "Not valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:14:00] SMILES Parse Error: unclosed ring for input: 'Cc1c(C(=O)OCC(=O)N2CCCC2=O)oc2ccccc1'\n"
     ]
    }
   ],
   "source": [
    "random_molecule_num = random.randint(0, len(molecules) - 1)\n",
    "random_molecule = molecules[random_molecule_num]\n",
    "\n",
    "print(random_molecule)\n",
    "print(is_valid_smiles(random_molecule))\n",
    "\n",
    "wrong_molecule = random_molecule[0:-1]\n",
    "\n",
    "print(wrong_molecule)\n",
    "print(is_valid_smiles(wrong_molecule))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13fabc6",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Hieronder staat een functie die een array aan moleculen kan omzetten in losse characters om deze erna naar nummers om te zetten zodat mijn modellen deze nummers kunnen lezen en een predictie op maken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebdfda46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['C', 'C', 'C', '(', 'C', ')', '(', 'C', ')', 'B', 'r'], ['C', 'C', 'C', 'N', '(', 'C', 'C', 'c', '1', 'c', 'c', 'c', 'c', '(', '-', 'c', '2', 'c', 'c', 'c', 'c', 'c', '2', ')', 'c', '1', ')', 'C', '(', '=', 'O', ')', 'C', '1', 'O', 'C', '(', 'C', '(', '=', 'O', ')', 'O', ')', '=', 'C', 'C', '(', 'N', ')', 'C', '1', 'N', 'C', '(', 'C', ')', '=', 'O'], ['O', 'c', '1', 'c', 'c', 'c', '(', 'C', '2', 'C', 'C', '(', 'c', '3', 'c', 'c', 'c', 'c', 'c', '3', ')', '=', 'N', 'N', '2', 'C', '(', '=', 'S', ')', 'N', 'c', '2', 'c', 'c', 'c', 'c', 'c', '2', ')', 'c', 'c', '1'], ['C', 'C', '1', '(', 'C', ')', 'O', 'C', 'C', '2', 'O', 'C', '3', '(', 'C', '4', 'O', 'C', '(', 'C', ')', '(', 'C', ')', 'O', 'C', '4', 'C', 'O', ')', 'O', 'C', '(', 'C', ')', '(', 'C', ')', 'O', 'C', '3', 'C', '2', 'O', '1'], ['C', 'O', 'C', '(', '=', 'O', ')', 'c', '1', 'c', 'c', '(', 'C', '(', '=', 'C', 'C', 'C', 'C', 'C', '(', '=', 'O', ')', 'S', 'C', ')', 'c', '2', 'c', 'c', '(', 'C', 'l', ')', 'c', '(', 'O', 'C', ')', 'c', '(', 'C', '(', '=', 'O', ')', 'O', 'C', ')', 'c', '2', ')', 'c', 'c', '(', 'C', 'l', ')', 'c', '1', 'O', 'C']]\n"
     ]
    }
   ],
   "source": [
    "def molecules_to_tokens(molecules):\n",
    "    tokens = list()\n",
    "    for molecule in molecules:\n",
    "        tokens.append(list(molecule))\n",
    "    return tokens\n",
    "\n",
    "tokens = molecules_to_tokens(molecules[0:5])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efcb3ae",
   "metadata": {},
   "source": [
    "Hieronder heb ik een functie geschreven die een set aan tokens pakt en hieruit alle unieke characters haalt, deze worden vervlgens omgezet naar een dictionary die er nummers aan koppelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22dd84bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens: {'B', 'S', 'l', '(', 'C', '-', 'N', ')', 'r', '1', '2', '=', 'c', '4', '3', 'O'}\n",
      "Sorted tokens: ['(', ')', '-', '1', '2', '3', '4', '=', 'B', 'C', 'N', 'O', 'S', 'c', 'l', 'r']\n",
      "Character to index mapping: {'(': 1, ')': 2, '-': 3, '1': 4, '2': 5, '3': 6, '4': 7, '=': 8, 'B': 9, 'C': 10, 'N': 11, 'O': 12, 'S': 13, 'c': 14, 'l': 15, 'r': 16}\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "def create_vocab(tokens):\n",
    "    unique_tokens = set(chain.from_iterable(tokens))\n",
    "    print(f\"Unique tokens: {unique_tokens}\")\n",
    "\n",
    "    sorted_tokens = sorted(unique_tokens)\n",
    "    print(f\"Sorted tokens: {sorted_tokens}\")\n",
    "\n",
    "    char_to_index = {}\n",
    "    for i, token in enumerate(sorted_tokens, start=1):\n",
    "        char_to_index[token] = i\n",
    "    print(f\"Character to index mapping: {char_to_index}\")\n",
    "    return char_to_index\n",
    "\n",
    "vocab = create_vocab(tokens)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e4259",
   "metadata": {},
   "source": [
    "Nu dat ik een vocabulary heb gemaakt kan ik de moleculen die ik heb omgezet naar tokens veranderen in cijfers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4183df2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 10, 10, 1, 10, 2, 1, 10, 2, 9, 16], [10, 10, 10, 11, 1, 10, 10, 14, 4, 14, 14, 14, 14, 1, 3, 14, 5, 14, 14, 14, 14, 14, 5, 2, 14, 4, 2, 10, 1, 8, 12, 2, 10, 4, 12, 10, 1, 10, 1, 8, 12, 2, 12, 2, 8, 10, 10, 1, 11, 2, 10, 4, 11, 10, 1, 10, 2, 8, 12], [12, 14, 4, 14, 14, 14, 1, 10, 5, 10, 10, 1, 14, 6, 14, 14, 14, 14, 14, 6, 2, 8, 11, 11, 5, 10, 1, 8, 13, 2, 11, 14, 5, 14, 14, 14, 14, 14, 5, 2, 14, 14, 4], [10, 10, 4, 1, 10, 2, 12, 10, 10, 5, 12, 10, 6, 1, 10, 7, 12, 10, 1, 10, 2, 1, 10, 2, 12, 10, 7, 10, 12, 2, 12, 10, 1, 10, 2, 1, 10, 2, 12, 10, 6, 10, 5, 12, 4], [10, 12, 10, 1, 8, 12, 2, 14, 4, 14, 14, 1, 10, 1, 8, 10, 10, 10, 10, 10, 1, 8, 12, 2, 13, 10, 2, 14, 5, 14, 14, 1, 10, 15, 2, 14, 1, 12, 10, 2, 14, 1, 10, 1, 8, 12, 2, 12, 10, 2, 14, 5, 2, 14, 14, 1, 10, 15, 2, 14, 4, 12, 10]]\n"
     ]
    }
   ],
   "source": [
    "numeric_tokens = [[vocab[char] for char in molecule] for molecule in tokens]\n",
    "print(numeric_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72ae057",
   "metadata": {},
   "source": [
    "### Padding en sifting\n",
    "Padding en spacing is nodig omdat een AI model elke input van de zelfde lengte moet hebben, een molecuul van bijvoorbeeld een lengte van 10 moet ook samen kunnen met een molecuul van bijvoorbeeld een lengte van 40. Ik doe dit door te kijken welk molecuul in de numeric array de langste is en bij de andere nullen toevoegen tot ik aan de zelfde lengte zit. Bij mijn toepassen zit wel nog een fout, als ik op de hele database ga trainen dan kan het zijn dat er grotere moleculen voorkomen en ik dus mijn model moet aanpassen.\n",
    "\n",
    "Shifting is het toevoegen van een 0 aan het begin van een molecuul bijvoorbeeld een molecuul als C=O (in SMILES) is dan in tokens \"C, =, O\" en in nummers [3, 2, 4] doormiddel van shiften worden de nummers dan [0, 3, 2] en weet het model dat die de 0 moet gaan voorspellen, dit is hoe een RNN sequentialy kan leren.\n",
    "\n",
    "Als ik shifting gebruik moet ik ook één bij de lengte toevoegen om rekening te houden met het extra getal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e9c31b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest molecule: 64\n"
     ]
    }
   ],
   "source": [
    "def find_longest_molecule(molecules):\n",
    "    longest = 0\n",
    "    for n in range(len(molecules)):\n",
    "        if len(molecules[n]) > longest:\n",
    "            longest = len(molecules[n])\n",
    "    return longest + 1\n",
    "\n",
    "longest = find_longest_molecule(numeric_tokens)\n",
    "print(f\"Longest molecule: {longest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6795bd91",
   "metadata": {},
   "source": [
    "Nu is het tijd om de input en de targets voor het model te maken, dit bestaan uit input_tokens die de numeric token zijn maar dan met een extra 0 ervoor, dit is om het model te laten leren wat erna komt. Ook maak ik hier de target voor het model, dit zijn dan de numeric values maar met een extra 0 om de offset te houden en de lengte het zelfde te houden. Een RNN heeft dan ook de zelfde lengte input en output nodig."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1291e251",
   "metadata": {},
   "source": [
    "Extra, het is ook een oplossing om een < END > en < START > token te maken om zo nog beter aan te geven wanneer het model moet starten en stoppen met de sequence. Dit kan ik dan in de vocabulary erbij zetten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e108257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: [[0, 10, 10, 10, 1, 10, 2, 1, 10, 2, 9, 16], [0, 10, 10, 10, 11, 1, 10, 10, 14, 4, 14, 14, 14, 14, 1, 3, 14, 5, 14, 14, 14, 14, 14, 5, 2, 14, 4, 2, 10, 1, 8, 12, 2, 10, 4, 12, 10, 1, 10, 1, 8, 12, 2, 12, 2, 8, 10, 10, 1, 11, 2, 10, 4, 11, 10, 1, 10, 2, 8, 12], [0, 12, 14, 4, 14, 14, 14, 1, 10, 5, 10, 10, 1, 14, 6, 14, 14, 14, 14, 14, 6, 2, 8, 11, 11, 5, 10, 1, 8, 13, 2, 11, 14, 5, 14, 14, 14, 14, 14, 5, 2, 14, 14, 4], [0, 10, 10, 4, 1, 10, 2, 12, 10, 10, 5, 12, 10, 6, 1, 10, 7, 12, 10, 1, 10, 2, 1, 10, 2, 12, 10, 7, 10, 12, 2, 12, 10, 1, 10, 2, 1, 10, 2, 12, 10, 6, 10, 5, 12, 4], [0, 10, 12, 10, 1, 8, 12, 2, 14, 4, 14, 14, 1, 10, 1, 8, 10, 10, 10, 10, 10, 1, 8, 12, 2, 13, 10, 2, 14, 5, 14, 14, 1, 10, 15, 2, 14, 1, 12, 10, 2, 14, 1, 10, 1, 8, 12, 2, 12, 10, 2, 14, 5, 2, 14, 14, 1, 10, 15, 2, 14, 4, 12, 10]]\n",
      "Target tokens: [[10, 10, 10, 1, 10, 2, 1, 10, 2, 9, 16, 0], [10, 10, 10, 11, 1, 10, 10, 14, 4, 14, 14, 14, 14, 1, 3, 14, 5, 14, 14, 14, 14, 14, 5, 2, 14, 4, 2, 10, 1, 8, 12, 2, 10, 4, 12, 10, 1, 10, 1, 8, 12, 2, 12, 2, 8, 10, 10, 1, 11, 2, 10, 4, 11, 10, 1, 10, 2, 8, 12, 0], [12, 14, 4, 14, 14, 14, 1, 10, 5, 10, 10, 1, 14, 6, 14, 14, 14, 14, 14, 6, 2, 8, 11, 11, 5, 10, 1, 8, 13, 2, 11, 14, 5, 14, 14, 14, 14, 14, 5, 2, 14, 14, 4, 0], [10, 10, 4, 1, 10, 2, 12, 10, 10, 5, 12, 10, 6, 1, 10, 7, 12, 10, 1, 10, 2, 1, 10, 2, 12, 10, 7, 10, 12, 2, 12, 10, 1, 10, 2, 1, 10, 2, 12, 10, 6, 10, 5, 12, 4, 0], [10, 12, 10, 1, 8, 12, 2, 14, 4, 14, 14, 1, 10, 1, 8, 10, 10, 10, 10, 10, 1, 8, 12, 2, 13, 10, 2, 14, 5, 14, 14, 1, 10, 15, 2, 14, 1, 12, 10, 2, 14, 1, 10, 1, 8, 12, 2, 12, 10, 2, 14, 5, 2, 14, 14, 1, 10, 15, 2, 14, 4, 12, 10, 0]]\n"
     ]
    }
   ],
   "source": [
    "def shift_tokens(numeric_tokens, shift_num):\n",
    "    shifted_tokens = []\n",
    "    for molecule in numeric_tokens:\n",
    "        molecule = [shift_num] + molecule    \n",
    "        shifted_tokens.append(molecule)\n",
    "    return shifted_tokens\n",
    "\n",
    "def add_end_padding(numeric_tokens, padding_value):\n",
    "    padded_tokens = []\n",
    "    for molecule in numeric_tokens:\n",
    "        molecule = molecule + [padding_value]\n",
    "        padded_tokens.append(molecule)\n",
    "    return padded_tokens\n",
    "\n",
    "\n",
    "\n",
    "input_tokens = shift_tokens(numeric_tokens, 0)\n",
    "target_tokens = add_end_padding(numeric_tokens, 0)\n",
    "print(f\"Input tokens: {input_tokens}\")\n",
    "print(f\"Target tokens: {target_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09da41e",
   "metadata": {},
   "source": [
    "Nu moet ik alleen nog zorgen dat alle inputs en targets daadwerkelijk de zelfde lengte hebben als het langste molecuul. Dit is heel makkelijk te doen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb959afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens after padding: [[0, 10, 10, 10, 1, 10, 2, 1, 10, 2, 9, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 10, 10, 10, 11, 1, 10, 10, 14, 4, 14, 14, 14, 14, 1, 3, 14, 5, 14, 14, 14, 14, 14, 5, 2, 14, 4, 2, 10, 1, 8, 12, 2, 10, 4, 12, 10, 1, 10, 1, 8, 12, 2, 12, 2, 8, 10, 10, 1, 11, 2, 10, 4, 11, 10, 1, 10, 2, 8, 12, 0, 0, 0, 0], [0, 12, 14, 4, 14, 14, 14, 1, 10, 5, 10, 10, 1, 14, 6, 14, 14, 14, 14, 14, 6, 2, 8, 11, 11, 5, 10, 1, 8, 13, 2, 11, 14, 5, 14, 14, 14, 14, 14, 5, 2, 14, 14, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 10, 10, 4, 1, 10, 2, 12, 10, 10, 5, 12, 10, 6, 1, 10, 7, 12, 10, 1, 10, 2, 1, 10, 2, 12, 10, 7, 10, 12, 2, 12, 10, 1, 10, 2, 1, 10, 2, 12, 10, 6, 10, 5, 12, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 10, 12, 10, 1, 8, 12, 2, 14, 4, 14, 14, 1, 10, 1, 8, 10, 10, 10, 10, 10, 1, 8, 12, 2, 13, 10, 2, 14, 5, 14, 14, 1, 10, 15, 2, 14, 1, 12, 10, 2, 14, 1, 10, 1, 8, 12, 2, 12, 10, 2, 14, 5, 2, 14, 14, 1, 10, 15, 2, 14, 4, 12, 10]]\n",
      "tokens after padding: [[10, 10, 10, 1, 10, 2, 1, 10, 2, 9, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 10, 10, 11, 1, 10, 10, 14, 4, 14, 14, 14, 14, 1, 3, 14, 5, 14, 14, 14, 14, 14, 5, 2, 14, 4, 2, 10, 1, 8, 12, 2, 10, 4, 12, 10, 1, 10, 1, 8, 12, 2, 12, 2, 8, 10, 10, 1, 11, 2, 10, 4, 11, 10, 1, 10, 2, 8, 12, 0, 0, 0, 0, 0], [12, 14, 4, 14, 14, 14, 1, 10, 5, 10, 10, 1, 14, 6, 14, 14, 14, 14, 14, 6, 2, 8, 11, 11, 5, 10, 1, 8, 13, 2, 11, 14, 5, 14, 14, 14, 14, 14, 5, 2, 14, 14, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 10, 4, 1, 10, 2, 12, 10, 10, 5, 12, 10, 6, 1, 10, 7, 12, 10, 1, 10, 2, 1, 10, 2, 12, 10, 7, 10, 12, 2, 12, 10, 1, 10, 2, 1, 10, 2, 12, 10, 6, 10, 5, 12, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 12, 10, 1, 8, 12, 2, 14, 4, 14, 14, 1, 10, 1, 8, 10, 10, 10, 10, 10, 1, 8, 12, 2, 13, 10, 2, 14, 5, 14, 14, 1, 10, 15, 2, 14, 1, 12, 10, 2, 14, 1, 10, 1, 8, 12, 2, 12, 10, 2, 14, 5, 2, 14, 14, 1, 10, 15, 2, 14, 4, 12, 10, 0]]\n"
     ]
    }
   ],
   "source": [
    "padding_num = 0\n",
    "def pad_tokens(input_tokens):\n",
    "    for molecule in input_tokens:\n",
    "        if len(molecule) < longest:\n",
    "            molecule += [padding_num] * (longest - len(molecule))\n",
    "    print(f\"tokens after padding: {input_tokens}\")\n",
    "    return input_tokens\n",
    "\n",
    "padded_inputs = pad_tokens(input_tokens)\n",
    "padded_targets = pad_tokens(target_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d163d5",
   "metadata": {},
   "source": [
    "Ik ben nu ook overgestapt naar een NP array en ik heb de inputs X genoemd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "295a2d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 10 10 10  1 10  2  1 10  2  9 16  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 10 10 10 11  1 10 10 14  4 14 14 14 14  1  3 14  5 14 14 14 14 14  5\n",
      "   2 14  4  2 10  1  8 12  2 10  4 12 10  1 10  1  8 12  2 12  2  8 10 10\n",
      "   1 11  2 10  4 11 10  1 10  2  8 12  0  0  0  0]\n",
      " [ 0 12 14  4 14 14 14  1 10  5 10 10  1 14  6 14 14 14 14 14  6  2  8 11\n",
      "  11  5 10  1  8 13  2 11 14  5 14 14 14 14 14  5  2 14 14  4  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 10 10  4  1 10  2 12 10 10  5 12 10  6  1 10  7 12 10  1 10  2  1 10\n",
      "   2 12 10  7 10 12  2 12 10  1 10  2  1 10  2 12 10  6 10  5 12  4  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 10 12 10  1  8 12  2 14  4 14 14  1 10  1  8 10 10 10 10 10  1  8 12\n",
      "   2 13 10  2 14  5 14 14  1 10 15  2 14  1 12 10  2 14  1 10  1  8 12  2\n",
      "  12 10  2 14  5  2 14 14  1 10 15  2 14  4 12 10]]\n",
      "[[10 10 10  1 10  2  1 10  2  9 16  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [10 10 10 11  1 10 10 14  4 14 14 14 14  1  3 14  5 14 14 14 14 14  5  2\n",
      "  14  4  2 10  1  8 12  2 10  4 12 10  1 10  1  8 12  2 12  2  8 10 10  1\n",
      "  11  2 10  4 11 10  1 10  2  8 12  0  0  0  0  0]\n",
      " [12 14  4 14 14 14  1 10  5 10 10  1 14  6 14 14 14 14 14  6  2  8 11 11\n",
      "   5 10  1  8 13  2 11 14  5 14 14 14 14 14  5  2 14 14  4  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [10 10  4  1 10  2 12 10 10  5 12 10  6  1 10  7 12 10  1 10  2  1 10  2\n",
      "  12 10  7 10 12  2 12 10  1 10  2  1 10  2 12 10  6 10  5 12  4  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [10 12 10  1  8 12  2 14  4 14 14  1 10  1  8 10 10 10 10 10  1  8 12  2\n",
      "  13 10  2 14  5 14 14  1 10 15  2 14  1 12 10  2 14  1 10  1  8 12  2 12\n",
      "  10  2 14  5  2 14 14  1 10 15  2 14  4 12 10  0]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(padded_inputs)\n",
    "print(X)\n",
    "y = np.array(target_tokens)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce251202",
   "metadata": {},
   "source": [
    "### One-Hot encoding\n",
    "Bij dit stukje one-hot encoding ben ik zelf een tijdje vastgelopen, ik snapte wel wat het deed maar niet waarom het nodig is. One-hot encoding is het process van het uitleggen wat de data is in verglijken met elkaar. Een molecuul zou kunnen zijn: \"C\", \"=\", \"0\" : 1, 2, 3 maar als ik deze data direct in een model zou stoppen dan kan het model een verband gaat leggen tussen de hoeveelheid en de afstand van de cijfers, bijvoorbeeld 1 is dichter bij 2 en verder weg van 3. Er wordt dan een verband gelegd tussen de cijfers die eigenlijk alleen maar symbolen voorstellen. One-hot encoding maakt deze nummers een 2d array: 1, 2, 3 wordt dan:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba8bc746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0], [0, 1, 0], [0, 0, 1]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e4d657",
   "metadata": {},
   "source": [
    "In het bovenstaande heb ik de lengte van de vocabulary, in dit geval 3, gepakt en heb op de die het getal van het atoom aan gaf een 1 gezet. Dit zorgt ervoor dat er geen verband meer is tussen de verschillende getallen, alles is een 1. Hieronder staat de code die ik gebruik om dit op mijn moleculen te doen. tensorflow.keras.utils heeft hier een functie voor die dit automatisch doet voor een getal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c956b45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 64, 17)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def one_hot_encode_tokens(tokens):\n",
    "    return to_categorical(tokens, num_classes=len(vocab) + 1)\n",
    "\n",
    "X_oh = one_hot_encode_tokens(X)\n",
    "y_oh = one_hot_encode_tokens(y)\n",
    "print(X_oh.shape)\n",
    "print(X_oh[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6f9687",
   "metadata": {},
   "source": [
    "### Model bouwen\n",
    "Nu is het dan ook eindelijk tijd voor het bouwen van het model. Sequential is de manier waarop het model gebouwd kan worden, in dit geval kun je dus een model layer voor layer opbouwen. LSTM is Long Short-Term Memory, dit is de laag in het RNN die er voor zorgt dat sequential leren mogelijk is. Dense is een layer die ik kan toevoegen die standaard met alle layers ervoor en erna connect, een soort tussenlayer dus. En de laatste import, Timedustributed is een layer die gelijk is aan Dense, deze layer doet ook elk punt van 2 layers met elkaar verbinden. TimeDistrubuted zorgt ervoor dat je een output kan krijgen in elke stap van je process wat nodig is voor het nakijken van gegenereede sequenties.\n",
    "\n",
    "Bij deze stukjes code heb ik ook de import per code vak gezet, het kan zijn dat dit trainen een tijdje duurt en je dus ook liever niet zit te wachten tot er een klaar is, door de imports per code blok te doen kun je deze dus ook apart runnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f7a9cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847f171c",
   "metadata": {},
   "source": [
    "Tijdens het maken van dit project ben ik op een extra Model uitgekomen GRU (Gated Recurrent Unit), dit is speciaal gemaakt voor het genereren van Sequenties maar is iets simpeler en sneller om mee te leren dan LSTM. Maar ondanks dat het simpeler en sneller is zou het nog steeds ongeveer het zelfde moeten presteren. Ik heb er daarom ook voor gekozen om hier mee te gaan beginnen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdc94f6",
   "metadata": {},
   "source": [
    "Ik heb dus hieronder het model gebouwd maar wat doet alles nu? Voor het bouwen van het model ben ik begonnen met een GRU layer met 128 Units (neurons) dit heb ik gekozen omdat het over het algemeen aangeraden wordt om met deze hoeveelheid te beginnen. De hoeveelheid van deze units bepaald eigenlijk hoe groot de memory van het model is, meer units betekent meer learning power maar ook meer kans op overfitten. return_sequences zorgt ervoor dat ik ook een sequence terugkrijg in plaats van maar 1 uitkomst. Input_shape is een array omdat ik een molecuul dus meerdere inputs tegelijk wil geven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2e07deb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, GRU\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = Sequential([\n",
    "    GRU(128, return_sequences=True, input_shape=(X_oh.shape[1], vocab_size)),\n",
    "    TimeDistributed(Dense(vocab_size, activation=\"softmax\")) \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0b0cce",
   "metadata": {},
   "source": [
    "Nadat het model is gebouwd moet dit ook compiled worden, het compiled van het model doet eigenlijk vast zetten hoe het model eruit ziet en de manier van leren ingeven. Met de loss function kijk je hoe fout het model is vergleken met de labels die je hebt meegegeven, categorical_crossentropy is speciaal gemaakt voor sequences dus goed voor dit project. Een lage loss is goed en een hoge loss is slecht. De optimizer functie bepaald hoe de weights van het trainen worden veranderd ten opzichte van de loss. Het model gaat dus als volgt trainen, de input is een molecuul en hierop word een gok gedaan doormiddel van de weights en bias. Na deze gok word er door de loss function gekeken hoe ver deze gok naast het getal zit wat het moet zijn. Doormiddel van de optimizer worden de biases in het model aangepast en dit wordt voor elk training molecuul gedaan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cded0674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">56,064</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,064</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m56,064\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │         \u001b[38;5;34m2,064\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,128</span> (227.06 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m58,128\u001b[0m (227.06 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,128</span> (227.06 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m58,128\u001b[0m (227.06 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e03f99",
   "metadata": {},
   "source": [
    "Bij het compilen van het model is mij opgevallen dat deze fysiek word opgeslagen, hieronder een functie om deze cache te clearen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f14f8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      2\u001b[39m tf.keras.backend.clear_session()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4399d8af",
   "metadata": {},
   "source": [
    "Nu het trainen van het model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c19b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
